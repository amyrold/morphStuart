---
title: "morphStuart"
author: "Aaron Myrold"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```
#==============================================================================#
# PROJECT INTRODUCTION

#==============================================================================#
# SETUP
## Loading Libraries
```{r Libraries, include=FALSE}
# load libraries needed for our analyses
# 1 ggplot2
if (!require("ggplot2")) install.packages("ggplot2")
library(ggplot2)

# 2 dplyr
if (!require("dplyr")) install.packages("dplyr")
library(dplyr)

# 3 tidyr
if (!require("tidyr")) install.packages("tidyr")
library(tidyr)

# 4 stringr
if (!require("stringr")) install.packages("stringr")
library(stringr)

# 5 tibble
if (!require("tibble")) install.packages("tibble")
library(tibble)

# 6 rioja
if (!require("rioja")) install.packages("rioja")
library(rioja)

# 6 vegan
if (!require("vegan")) install.packages("vegan")
library(vegan)

```

## Folder Management
```{r}
# This is a function to initialize a new project
# It creates required subfolders (see defaults) 
# It outputs an object that stores the relative paths to these sub-folders

# If calling the function within the working directory of the project, project_number = 0
# this will create necessary sub-folders in current working directory

# If creating a new root folder for the project, provide project_number >=1 and a project_name
# this will create a folder "pX.project_name/" to store all sub-folders
new_project <- function(project_number, project_name = NULL, subfolders = NULL, wk.dir = getwd()) {
  # Set default subfolders
  default_subfolders <- c("data", "scripts", "output", "docs")
  
  # Use provided sub-folders or defaults if none are provided
  if (is.null(subfolders)) {
    subfolders <- default_subfolders
  }
  
  # Initialize project paths list
  project_paths <- list()
  
  # Check if this is a "root directory" project (project_number = 0)
  is_root_project <- project_number == 0
  
  if (is_root_project) {
    # Use working directory as the base path for root projects
    base_path <- wk.dir
    project_paths$main <- wk.dir
    message("Creating folders directly in working directory: ", wk.dir)
  } else {
    # For normal projects, validate project name is provided
    if (is.null(project_name)) {
      stop("Project name is required when project_number is not 0")
    }
    
    # Create project folder name
    project_folder <- paste0("p", project_number, ".", project_name)
    
    # Full path for the project folder
    project_path <- file.path(wk.dir, project_folder)
    
    # Create the project folder if it doesn't exist
    if (!file.exists(project_path)) {
      dir.create(project_path)
      message("Created project folder: ", project_path)
    } else {
      message("Project folder already exists: ", project_path)
    }
    
    # Set base path for subfolders
    base_path <- project_path
    project_paths$main <- project_path
  }
  
  # Create sub-folders
  for (sub in subfolders) {
    # Determine subfolder name based on project type
    if (is_root_project) {
      subfolder_name <- sub
    } else {
      subfolder_name <- paste0("p", project_number, ".", sub)
    }
    
    # Full path for the subfolder
    subfolder_path <- file.path(base_path, subfolder_name)
    
    # Create sub-folder if it doesn't exist
    if (!file.exists(subfolder_path)) {
      dir.create(subfolder_path, recursive = TRUE)  # Added recursive=TRUE for nested paths
      message("Created subfolder: ", subfolder_path)
    } else {
      message("Subfolder already exists: ", subfolder_path)
    }
    
    # Add the sub-folder path to the list
    project_paths[[sub]] <- paste0(subfolder_path, "/")
  }
  
  # Return the list of paths as an object
  return(project_paths)
}

```

## Initialize Directories
```{r}
# Initialize new project directories
p <- new_project(0, subfolders = c("1.data", "1.data/a.raw", "1.data/b.merged",
                                   "1.data/c.flagged","1.data/d.processed", 
                                   "2.scripts", "3.outputs", "4.docs"))
cat("✓ Project directories initialized\n")
```

## Source Functions
```{r Load Validation Scripts, include=FALSE}
# Source the validation and logging functions
source(paste0(p$`2.scripts`,"validation_functions.R"))
source(paste0(p$`2.scripts`,"quality_assessment.R"))
source(paste0(p$`2.scripts`,"project_validation.R"))

cat("✓ All validation and logging functions loaded\n")
```

## Global Variables
```{r Global Variables, include=FALSE}
# store current user's working directory
wk.dir <- getwd()
# Set seed for reproducibility
set.seed(333)

# Initialize the logging system
processing_log <- initialize_log("Fish Morphology Data Cleaning Pipeline v2.0")
```

#==============================================================================#
# FUNCTIONS
## variable_mapping
```{r}
variable_mapping <- function() {
  # Define measurement types
  continuous_vars <- c("SL", "CAV", "DS1", "DS2", "DS3", "LPT", 
                      "PSP.L", "PSP.R", "TPG", "ECT", "CLE", "PMX")
  
  count_vars <- c("MDF", "MAF", "MCV", "MAV", "MPT", "MPSP")
  
  binary_vars <- c("MDS1", "MDS2", "MDS3", "MDS1NA", 
                   "MDS2NA", "MDS3NA", "MPSPNA", "PGNA")
  
  # Return as a list
  return(list(
    continuous = continuous_vars,
    count = count_vars,
    binary = binary_vars,
    all = c(continuous_vars, count_vars, binary_vars)
  ))
}
```

## handle_special_cases
```{r}
handle_special_cases <- function(data) {
  # requires dplyr
  
  # Use dplyr to handle all the special cases with cleaner syntax
  cleaned <- data %>%
    as_tibble() %>%
    # Handle dorsal spine columns - use case_when for readable conditional logic
    mutate(
      # Dorsal spine 1
      DS1 = case_when(
        MDS1 == 0 & MDS1NA == 1 ~ NA_real_,
        MDS1 == 0 & MDS1NA == 0 & is.na(DS1) ~ 0,
        TRUE ~ DS1
      ),
      # Dorsal spine 2
      DS2 = case_when(
        MDS2 == 0 & MDS2NA == 1 ~ NA_real_,
        MDS2 == 0 & MDS2NA == 0 & is.na(DS2) ~ 0,
        TRUE ~ DS2
      ),
      # Dorsal spine 3
      DS3 = case_when(
        MDS3 == 0 & MDS3NA == 1 ~ NA_real_,
        MDS3 == 0 & MDS3NA == 0 & is.na(DS3) ~ 0,
        TRUE ~ DS3
      ),
      
      # Handle MPT (pre-dorsal pterygiophores)
      MPT = case_when(
        MPT == 0 & (MDS1 == 1 | MDS2 == 1 | MDS3 == 1) ~ NA_real_,
        TRUE ~ MPT
      ),
      
      # Handle pelvic spine columns
      PSP.L = case_when(
        MPSP == 0 & MPSPNA == 1 ~ NA_real_,
        TRUE ~ PSP.L
      ),
      PSP.R = case_when(
        MPSP == 0 & MPSPNA == 1 ~ NA_real_,
        TRUE ~ PSP.R
      )
    )
  
  # When PGNA column is available, apply similar logic to TPG
  if ("PGNA" %in% names(cleaned)) {
    cleaned <- cleaned %>%
      mutate(
        TPG = case_when(
          PGNA == 1 ~ NA_real_,
          TRUE ~ TPG
        )
      )
  }
  
  return(as.data.frame(cleaned))
}

```

## identify_overlaps
```{r}
identify_overlaps <- function(data, threshold = 0.05) {
  # Requires tidyr and dplyr
  
  # Get variable mappings
  var_map <- variable_mapping()
  
  # Step 1: Create separate dataframes for part and counterpart data
  part_data <- data %>% 
    filter(part_type == "P") %>%
    select(fish_id, part_type, row_id = n, Scale_10mm, all_of(var_map$all))
  
  cpart_data <- data %>% 
    filter(part_type == "C") %>%
    select(fish_id, part_type, row_id = n, Scale_10mm, all_of(var_map$all))
  
  # Step 2: For each fish and measurement, aggregate values to avoid many-to-many joins
  # This preserves information about which fish have duplicates without creating cartesian products
  part_agg <- part_data %>%
    pivot_longer(
      cols = all_of(var_map$all),
      names_to = "measure",
      values_to = "value"
    ) %>%
    filter(!is.na(value)) %>%
    group_by(fish_id, measure) %>%
    summarize(
      part_value = if(first(measure) %in% var_map$binary) max(value) else mean(value),
      part_scale = mean(Scale_10mm, na.rm = TRUE),
      has_multiple_values = n() > 1,
      .groups = "drop"
    )
  
  cpart_agg <- cpart_data %>%
    pivot_longer(
      cols = all_of(var_map$all),
      names_to = "measure",
      values_to = "value"
    ) %>%
    filter(!is.na(value)) %>%
    group_by(fish_id, measure) %>%
    summarize(
      cpart_value = if(first(measure) %in% var_map$binary) max(value) else mean(value),
      cpart_scale = mean(Scale_10mm, na.rm = TRUE),
      has_multiple_values = n() > 1,
      .groups = "drop"
    )
  
  # Step 3: Join part and counterpart data to find overlaps (now a one-to-one join)
  overlaps <- part_agg %>%
    inner_join(cpart_agg, by = c("fish_id", "measure"))
  
  # Step 4: Calculate differences with appropriate handling for each variable type
  overlaps <- overlaps %>%
    mutate(
      var_type = case_when(
        measure %in% var_map$continuous ~ "continuous",
        measure %in% var_map$count ~ "count",
        measure %in% var_map$binary ~ "binary",
        TRUE ~ "other"
      ),
      scale = coalesce((part_scale + cpart_scale) / 2, part_scale, cpart_scale),
      absolute_diff = abs(part_value - cpart_value),
      
      # Modified approach for different variable types
      relative_diff = case_when(
        # For continuous: relative to scale
        var_type == "continuous" & scale > 0 ~ absolute_diff / scale,
        # For count: proportion of max value
        var_type == "count" ~ absolute_diff / max(1, pmax(part_value, cpart_value)),
        # For binary: 0 or 1
        TRUE ~ as.numeric(absolute_diff > 0)
      ),
      
      # Flag criteria - customized by variable type
      exceeds_threshold = case_when(
        var_type == "continuous" ~ relative_diff > threshold,
        var_type == "count" ~ absolute_diff > 0, # Any difference in counts
        var_type == "binary" ~ FALSE, # Never flag binary (we'll take max)
        TRUE ~ FALSE
      ),
      
      # Additional flag if either part/counterpart has multiple values for this measure
      multiple_measurements = has_multiple_values.x | has_multiple_values.y
    )
  
  # Step 5: Determine fish to flag based on any exceeding threshold
  fish_to_flag <- overlaps %>%
    filter(exceeds_threshold) %>%
    select(fish_id) %>%
    distinct() %>%
    pull(fish_id)
  
  # Step 6: Split data into flagged and non-flagged groups
  # This maintains the original structure (multiple rows possible per fish/part)
  overlap_fish <- data %>%
    filter(fish_id %in% fish_to_flag)
  
  non_overlap_fish <- data %>%
    filter(!fish_id %in% fish_to_flag)
  
  # Return original format data frames
  return(list(
    overlap_fish = as.data.frame(overlap_fish),
    non_overlap_fish = as.data.frame(non_overlap_fish),
    overlap_metrics = as.data.frame(overlaps)
  ))
}

```
## merge_non_overlap
```{r}
merge_non_overlap <- function(data) {
  # requires dplyr and tidyr
  
  # Get variable mappings for specialized handling
  var_map <- variable_mapping()
  
  # Get unique fish IDs
  fish_ids <- unique(data$fish_id)
  
  # Define metadata columns (non-measurement columns)
  id_cols <- c("n", "ID", "fish_id", "part_type", "bin")
  measure_cols <- setdiff(names(data), id_cols)
  
  # Initialize empty dataframe for results
  result_list <- list()
  
  # Process each fish
  for (i in seq_along(fish_ids)) {
    id <- fish_ids[i]
    
    # Get all data for this fish
    fish_data <- data %>% 
      filter(fish_id == id)
    
    # Get part and counterpart data
    part_data <- fish_data %>% filter(part_type == "P")
    cpart_data <- fish_data %>% filter(part_type == "C")
    
    # Create a results row
    result_row <- data.frame(fish_id = id)
    
    # If only one part type exists, use that directly (but still aggregate if multiple rows)
    if (nrow(part_data) == 0) {
      # No part data, use aggregated counterpart data
      if (nrow(cpart_data) > 0) {
        # Use first row for metadata
        template_row <- cpart_data[1,]
        for (col in id_cols) {
          if (col != "part_type") {
            result_row[[col]] <- template_row[[col]]
          } else {
            result_row[[col]] <- "merged"
          }
        }
        
        # Aggregate counterpart measurements
        for (col in measure_cols) {
          var_type <- "continuous"  # default
          if (col %in% var_map$binary) var_type <- "binary"
          if (col %in% var_map$count) var_type <- "count"
          
          values <- cpart_data[[col]]
          values <- values[!is.na(values)]
          
          if (length(values) > 0) {
            if (var_type %in% c("binary", "count")) {
              result_row[[col]] <- max(values)
            } else {
              result_row[[col]] <- mean(values)
            }
          } else {
            result_row[[col]] <- NA
          }
        }
      }
      result_row$part_type <- "merged"
      result_row$merged <- TRUE
      result_list[[i]] <- result_row
      next
    } else if (nrow(cpart_data) == 0) {
      # No counterpart data, use aggregated part data
      # Use first row for metadata
      template_row <- part_data[1,]
      for (col in id_cols) {
        if (col != "part_type") {
          result_row[[col]] <- template_row[[col]]
        } else {
          result_row[[col]] <- "merged"
        }
      }
      
      # Aggregate part measurements
      for (col in measure_cols) {
        var_type <- "continuous"  # default
        if (col %in% var_map$binary) var_type <- "binary"
        if (col %in% var_map$count) var_type <- "count"
        
        values <- part_data[[col]]
        values <- values[!is.na(values)]
        
        if (length(values) > 0) {
          if (var_type %in% c("binary", "count")) {
            result_row[[col]] <- max(values)
          } else {
            result_row[[col]] <- mean(values)
          }
        } else {
          result_row[[col]] <- NA
        }
      }
      result_row$part_type <- "merged"
      result_row$merged <- TRUE
      result_list[[i]] <- result_row
      next
    }
    
    # Both part and counterpart exist - aggregate them separately then merge
    
    # Use first part row for metadata
    template_row <- part_data[1,]
    for (col in id_cols) {
      if (col != "part_type") {
        result_row[[col]] <- template_row[[col]]
      } else {
        result_row[[col]] <- "merged"
      }
    }
    result_row$merged <- TRUE
    
    # For each measurement column, aggregate first, then merge values prioritizing part over counterpart
    for (col in measure_cols) {
      # Determine variable type
      var_type <- "continuous"  # default
      if (col %in% var_map$binary) var_type <- "binary"
      if (col %in% var_map$count) var_type <- "count"
      
      # Aggregate part values
      part_vals <- part_data[[col]]
      part_vals <- part_vals[!is.na(part_vals)]
      part_val <- NA
      if (length(part_vals) > 0) {
        if (var_type %in% c("binary", "count")) {
          part_val <- max(part_vals)
        } else {
          part_val <- mean(part_vals)
        }
      }
      
      # Aggregate counterpart values
      cpart_vals <- cpart_data[[col]]
      cpart_vals <- cpart_vals[!is.na(cpart_vals)]
      cpart_val <- NA
      if (length(cpart_vals) > 0) {
        if (var_type %in% c("binary", "count")) {
          cpart_val <- max(cpart_vals)
        } else {
          cpart_val <- mean(cpart_vals)
        }
      }
      
      # Priority: part > counterpart
      if (!is.na(part_val)) {
        result_row[[col]] <- part_val
      } else if (!is.na(cpart_val)) {
        result_row[[col]] <- cpart_val
      } else {
        result_row[[col]] <- NA
      }
    }
    
    # Add to result list
    result_list[[i]] <- result_row
  }
  
  # Combine all results
  result <- bind_rows(result_list)
  
  # Ensure all original columns exist in output
  for (col in names(data)) {
    if (!(col %in% names(result))) {
      result[[col]] <- NA
    }
  }
  
  # Add merged flag if not already present
  if (!("merged" %in% names(result))) {
    result$merged <- TRUE
  }
  
  return(as.data.frame(result))
}
```

## create_review_list
```{r}
create_review_list <- function(overlap_results, use_threshold = TRUE) {
  # requires dplyr
  metrics <- overlap_results$overlap_metrics
  
  # Get variable mappings
  var_map <- variable_mapping()
  
  # First, optionally filter the metrics based on use_threshold parameter
  filtered_metrics <- if(use_threshold) {
    metrics %>% filter(exceeds_threshold == TRUE)
  } else {
    metrics
  }
  
  # Add more meaningful grouping and sorting
  summary <- filtered_metrics %>%
    # First summarize by fish and variable type
    group_by(fish_id, var_type) %>%
    summarize(
      num_overlapping = n(),
      # For continuous vars, calculate max difference
      max_relative_diff = if_else(
        first(var_type) == "continuous", 
        max(relative_diff, na.rm = TRUE), 
        NA_real_
      ),
      .groups = "drop"
    ) %>%
    # Then create a summary by fish
    group_by(fish_id) %>%
    summarize(
      continuous_diffs = sum(var_type == "continuous"),
      count_diffs = sum(var_type == "count"),
      binary_diffs = sum(var_type == "binary"),  # Should be 0 if use_threshold is TRUE
      total_diffs = n(),
      max_relative_diff = max(max_relative_diff, na.rm = TRUE),
      .groups = "drop"
    ) %>%
    # Sort by total differences
    arrange(desc(total_diffs), desc(continuous_diffs))
  
  # Create a more detailed breakdown by measurement type
  details <- filtered_metrics %>%
    arrange(fish_id, var_type, desc(relative_diff))
  
  return(list(
    summary = as.data.frame(summary),
    details = as.data.frame(details),
    filtered_metrics = as.data.frame(filtered_metrics),
    use_threshold = use_threshold
  ))
}
```

## export_review_list
```{r}
export_review_list <- function(review_results, file_prefix = "fish_review", 
                               output_dir = NULL) {
  # Create output directory if specified and it doesn't exist
  if (!is.null(output_dir)) {
    if (!dir.exists(output_dir)) {
      dir.create(output_dir, recursive = TRUE)
    }
    summary_path <- file.path(output_dir, paste0(file_prefix, "_summary.csv"))
    details_path <- file.path(output_dir, paste0(file_prefix, "_details.csv"))
  } else {
    summary_path <- paste0(file_prefix, "_summary.csv")
    details_path <- paste0(file_prefix, "_details.csv")
  }
  
  # Export files
  readr::write_csv(review_results$summary, summary_path)
  readr::write_csv(review_results$details, details_path)
  
  # Return file paths
  return(list(
    summary = summary_path,
    details = details_path
  ))
}

```

## validate_data
```{r}
# Add a validation function to ensure data quality
validate_data <- function(original, corrected, merged, overlaps) {
  # requires dplyr
  
  # Initialize results
  validation <- list()
  
  # Check if any rows were lost
  total_specimens_original <- length(unique(original$fish_id))
  total_specimens_after <- length(unique(c(
    merged$fish_id, 
    unique(overlaps$fish_id)
  )))
  
  validation$all_specimens_accounted_for <- 
    total_specimens_original == total_specimens_after
  
  validation$specimen_counts <- tibble(
    stage = c("Original", "Merged", "Flagged for review", "Total after processing"),
    count = c(
      total_specimens_original,
      length(unique(merged$fish_id)),
      length(unique(overlaps$fish_id)),
      total_specimens_after
    )
  )
  
  # Check special case handling
  special_case_check <- corrected %>%
    filter(
      (MDS1 == 0 & MDS1NA == 1 & !is.na(DS1)) |
      (MDS2 == 0 & MDS2NA == 1 & !is.na(DS2)) |
      (MDS3 == 0 & MDS3NA == 1 & !is.na(DS3)) |
      (MPT == 0 & (MDS1 == 1 | MDS2 == 1 | MDS3 == 1)) |
      (MPSP == 0 & MPSPNA == 1 & (!is.na(PSP.L) | !is.na(PSP.R)))
    )
  
  validation$special_cases_properly_handled <- nrow(special_case_check) == 0
  validation$problematic_rows <- special_case_check
  
  return(validation)
}

```

```{r}


```

```{r}


```

#==============================================================================#
# DATA IMPORT
```{r}
# Read in data and check initial structure
morph <- read.csv(paste0(p$`1.data/a.raw`, "020625_PitLMorph.csv"))
str(morph)

# ADD LOGGING:
processing_log <- log_step(
  processing_log, 
  step_name = "Data Import",
  input_data = morph,
  notes = "Initial import of raw morphology data"
)

# ADD VALIDATION:
validate_step(
  morph, 
  step_name = "Data Import",
  required_columns = c("n", "ID", "SL", "Scale_10mm"),
  custom_checks = fish_data_checks[c("count_variable_logic")]  # This check now works without fish_id
)

# ADD QUALITY ASSESSMENT:
initial_quality <- create_quality_report(morph, output_dir = paste0(p$`4.docs`, "reports"))
```

## ID Structure Analysis
```{r}
# Extract fish ID and part type
# ID format: VXXXXXX_LXXXX(A)_1_P/C.jpg or VXXXXXX_LXXXX(A)_1_P2.jpg
# where VXXXXXX is the specimen ID, LXXXX(A) is the bin/locality, and P/C indicates part/counterpart
morph_with_ids <- morph %>%
  mutate(
    fish_id = str_extract(ID, "^V\\d+"),                          # Extract VXXXXX part at beginning
    bin_raw = str_extract(ID, "L\\d+[A-Za-z]?"),                  # Extract LXXXX with optional letter
    # Split bin into letter prefix, number, and suffix
    bin_letter = str_extract(bin_raw, "^L"),
    bin_number = str_extract(bin_raw, "\\d+"),
    bin_suffix = str_extract(bin_raw, "[A-Za-z]$"),
    # Create standardized bin with zero padding
    bin = case_when(
      !is.na(bin_suffix) ~ paste0(bin_letter, str_pad(bin_number, 4, pad = "0"), bin_suffix),
      !is.na(bin_number) ~ paste0(bin_letter, str_pad(bin_number, 4, pad = "0")),
      TRUE ~ NA_character_
    ),
    # Extract P or C (with optional numbers) before .jpg, then standardize
    part_type_raw = str_extract(ID, "[PC]\\d*(?=\\.jpg$)"),      # Extract P, P2, P3, etc. or C before .jpg
    part_type = case_when(
      str_starts(part_type_raw, "P") ~ "P",                      # Convert P, P2, P3, etc. to just "P"
      part_type_raw == "C" ~ "C",                                # Keep C as is
      TRUE ~ part_type_raw                                       # Preserve any unexpected values
    )
  ) %>%
  select(-bin_raw, -bin_letter, -bin_number, -bin_suffix, -part_type_raw)  # Remove intermediate columns


# ADD LOGGING:
processing_log <- log_step(
  processing_log,
  step_name = "ID Structure Analysis", 
  input_data = morph,
  output_data = morph_with_ids,
  notes = "Extracted fish_id and part_type from ID column"
)

# Continue with updated ids
morph <- morph_with_ids
```

## Basic Quality Check
```{r}
# Check for implausible values in measurement columns
length_cols <- c("SL", "CAV", "DS1", "DS2", "DS3", "LPT", "PSP.L", "PSP.R", "TPG", "ECT", "CLE", "PMX")

# Check for negative values in length columns
negative_lengths <- morph %>%
  select(ID, all_of(length_cols)) %>%
  pivot_longer(cols = -ID, names_to = "measure", values_to = "value") %>%
  filter(value < 0)
length(negative_lengths$ID) # No negative measurements

```

## Flag missing Scale_10mm
```{r}
# Check for missing scale information
scale_summary <- morph %>%
  group_by(fish_id) %>%
  summarize(
    has_part_scale = any(!is.na(Scale_10mm[part_type == "P"])),
    has_cpart_scale = any(!is.na(Scale_10mm[part_type == "C"])),
    has_any_scale = has_part_scale | has_cpart_scale,
    .groups = "drop"
  )

# Get list of fish IDs missing scales
fish_missing_scales <- scale_summary %>%
  filter(!has_any_scale) %>%
  pull(fish_id)

# Create report of fish missing scale information
missing_scales_detail <- morph %>%
  filter(fish_id %in% fish_missing_scales) %>%
  select(fish_id, ID, part_type, Scale_10mm) %>%
  arrange(fish_id, part_type)

# Write the detailed report
write.csv(missing_scales_detail, 
          file = paste0(p$`1.data/c.flagged`, "/fish_missing_scales_detailed.csv"), 
          row.names = FALSE)

# Print summary of scale check
cat("Fish without any scale information:", 
    sum(!scale_summary$has_any_scale), 
    "out of", nrow(scale_summary), "\n")

# Split the dataset
morph_with_scales <- morph %>%
  filter(!fish_id %in% fish_missing_scales)

morph_without_scales <- morph %>%
  filter(fish_id %in% fish_missing_scales)

# ADD LOGGING:
processing_log <- log_step(
  processing_log,
  step_name = "Scale Filtering",
  input_data = morph,
  output_data = morph_with_scales,
  notes = paste("Removed", length(fish_missing_scales), "fish without scale measurements")
)

# Continue with the fish that have scale measurements
morph <- morph_with_scales

# Write the datasets to separate files for reference
write.csv(morph_with_scales, 
          file = paste0(p$`1.data/a.raw`, "/fish_with_scales.csv"), 
          row.names = FALSE)

write.csv(morph_without_scales, 
          file = paste0(p$`1.data/c.flagged`, "/fish_without_scales.csv"), 
          row.names = FALSE)
```

#==============================================================================#
# DATA PREPROCESSING
## Handle Special Cases
```{r Spine and Girdle Logic}

morph_corrected <- handle_special_cases(morph)

# ADD LOGGING:
processing_log <- log_step(
  processing_log,
  step_name = "Special Cases Handling",
  input_data = morph_with_scales,
  output_data = morph_corrected,
  notes = "Applied spine and girdle logic rules"
)

```

## Overlap Assesment
```{r}
overlap_results <- identify_overlaps(morph_corrected, threshold = 0.05)

# ADD LOGGING:
processing_log <- log_step(
  processing_log,
  step_name = "Overlap Assessment",
  input_data = morph_corrected,
  notes = paste("Identified", length(unique(overlap_results$overlap_fish$fish_id)), 
                "fish with conflicts")
)
# Summarize overlap findings
cat("Number of fish with overlapping measurements:", 
    length(unique(overlap_results$overlap_metrics$fish_id)), "\n")
cat("Number of fish without overlaps:", 
    length(unique(overlap_results$non_overlap_fish$fish_id)), "\n")

```

#==============================================================================#
# DATA CLEANING
## Merge Non-Overlapping Fish
```{r}
merged_non_overlap <- merge_non_overlap(overlap_results$non_overlap_fish)

# Write merged data for Non-Overlapping Fish
merged_file <- write.csv(merged_non_overlap, 
          file = paste0(p$`1.data/b.merged`, "/merged_non_overlap.csv"))

# ADD LOGGING:
processing_log <- log_step(
  processing_log,
  step_name = "Merge Non-Overlapping Fish",
  input_data = overlap_results$non_overlap_fish,
  output_data = merged_non_overlap,
  files_created = merged_file,
  notes = "Merged part and counterpart data"
)

```

## Flag Conflicting Measurements
```{r Flag Duplicates}
# Create review list with threshold filtering applied (default)
review_results <- create_review_list(overlap_results, use_threshold = TRUE)

# Or to see all overlaps without threshold filtering:
# review_results_all <- create_review_list(overlap_results, use_threshold = FALSE)

export_review_list(review_results, file_prefix = "fish_fossil_review",
                   output_dir = p$`1.data/c.flagged`)

# ADD LOGGING:
processing_log <- log_step(
  processing_log,
  step_name = "Flag Conflicting Measurements",
  input_data = overlap_results$overlap_fish,
  notes = paste("Created review lists for", nrow(review_results$summary), "fish")
)

```

## Visualize Results
```{r}
# Get the filtered metrics that only include real overlaps
filtered_metrics <- review_results$filtered_metrics

# Create a suffix for titles based on whether threshold filtering was used
threshold_text <- if(review_results$use_threshold) " (Exceeding Threshold)" else " (All Overlaps)"

# Visualization 1: Distribution of relative differences
ggplot(filtered_metrics, aes(x = relative_diff)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  geom_vline(xintercept = 0.05, color = "red", linetype = "dashed") +
  labs(title = paste0("Distribution of Relative Differences", threshold_text),
       x = "Relative Difference (proportion of scale)",
       y = "Count") +
  theme_minimal()

# Visualization 2: Count of overlaps by measurement type
overlap_by_measure <- filtered_metrics %>%
  group_by(measure) %>%
  summarise(
    count = n(),
    avg_diff = mean(relative_diff, na.rm = TRUE),
    .groups = "drop"
  )

ggplot(overlap_by_measure, aes(x = reorder(measure, count), y = count)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = paste0("Number of fish with duplicate measurements by type", threshold_text),
       x = "Measurement",
       y = "Count of fish with duplicates") 

# Visualization 3: Distribution of flags per fish
ggplot(review_results$summary, aes(x = total_diffs)) +
  geom_histogram(binwidth = 1, fill = "coral") +
  theme_minimal() +
  labs(title = paste0("Distribution of duplicate measurements per fish", threshold_text),
       x = "Number of duplicate measurements",
       y = "Count of fish")

# Visualization 4: Distribution of differences by variable type
ggplot(filtered_metrics, 
       aes(x = var_type, y = absolute_diff, fill = var_type)) +
  geom_boxplot() +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal() +
  labs(title = paste0("Distribution of Differences by Variable Type", threshold_text),
       x = "Variable Type", 
       y = "Absolute Difference",
       fill = "Variable Type") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
#==============================================================================#
# MERGE AGE ESTIMATES
## Format Age Data for Merging
```{r Format Age Data}
# Read age data (assuming it's already loaded as 'age')
age <- read.csv(paste0(p$`1.data/a.raw`, "PitLMorph_fieldorder.csv"))

# Convert L..SPEC to match bin formatting
age_formatted <- age %>%
  # Convert L..SPEC to character and format as bin
  mutate(
    # Handle the column name with dots - use backticks or rename
    L_SPEC_clean = `L..SPEC`,
    # Create bin column matching the morph data format: "L" + 4-digit zero-padded number
    bin = paste0("L", str_pad(as.character(L_SPEC_clean), 4, pad = "0")),
    # Convert relevant columns to numeric for analysis
    CSTRAT = as.numeric(CSTRAT),
    ISTRAT = as.numeric(ISTRAT), 
    YEAR = as.numeric(YEAR),
    INT = as.numeric(INT)
  ) %>%
  # Select only the columns we need for merging
  select(bin, CSTRAT, ISTRAT, YEAR, INT) %>%
  # Remove any rows where bin couldn't be created (NA values in L..SPEC)
  filter(!is.na(bin), bin != "LNA")

# Check the formatting
head(age_formatted)


```

## Merge with Morphological Data
```{r Merge Age Data}
# Merge age data with the processed morphological data
# Assuming you have your merged_non_overlap data from the previous processing

# Read the merged morphological data if not already in memory
if (!exists("merged_non_overlap")) {
  merged_non_overlap <- read.csv(paste0(p$`1.data/b.merged`, "/merged_non_overlap.csv"))
}

# Perform the merge
morph_with_age <- merged_non_overlap %>%
  left_join(age_formatted, by = "bin")

# ADD LOGGING:
processing_log <- log_step(
  processing_log,
  step_name = "Age Data Merging",
  input_data = merged_non_overlap,
  output_data = morph_with_age,
  notes = paste("Merged age data for", sum(!is.na(morph_with_age$CSTRAT)), "specimens")
)

# EXPORT COMPREHENSIVE PROCESSING LOG:
log_files <- export_log(processing_log, output_dir = paste0(p$`4.docs`, "reports"))


# Check merge results
merge_summary <- morph_with_age %>%
  summarise(
    total_specimens = n(),
    specimens_with_age_data = sum(!is.na(CSTRAT)),
    specimens_with_istrat = sum(!is.na(ISTRAT)),
    specimens_with_year = sum(!is.na(YEAR)),
    specimens_with_int = sum(!is.na(INT)),
    percent_with_age_data = round(100 * sum(!is.na(CSTRAT)) / n(), 1)
  )

print("Merge Summary:")
print(merge_summary)

# Check which bins from morphological data don't have age matches
missing_age_bins <- morph_with_age %>%
  filter(is.na(CSTRAT)) %>%
  select(bin) %>%
  distinct() %>%
  pull(bin)

if(length(missing_age_bins) > 0) {
  cat("\nBins in morphological data without age matches:\n")
  print(missing_age_bins)
}

# Check which bins from age data don't have morphological matches
available_age_bins <- unique(age_formatted$bin)
used_age_bins <- unique(morph_with_age$bin[!is.na(morph_with_age$CSTRAT)])
unused_age_bins <- setdiff(available_age_bins, used_age_bins)

if(length(unused_age_bins) > 0) {
  cat("\nBins in age data without morphological matches:\n")
  print(head(unused_age_bins, 20))  # Show first 20
  if(length(unused_age_bins) > 20) {
    cat("... and", length(unused_age_bins) - 20, "more\n")
  }
}
```

## Save Merged Dataset
```{r Save Merged Data}
# Write the merged dataset
write.csv(morph_with_age, 
          file = paste0(p$`1.data/b.merged`, "morph_with_age.csv"),
          row.names = FALSE)

# Create a summary report of the merge
merge_report <- list(
  merge_summary = merge_summary,
  missing_age_bins = missing_age_bins,
  unused_age_bins = unused_age_bins,
  total_age_records = nrow(age_formatted),
  total_morph_records = nrow(merged_non_overlap),
  final_merged_records = nrow(morph_with_age)
)

# Save the merge report
saveRDS(merge_report, file = paste0(p$`3.outputs`, "/age_merge_report.rds"))

cat("\nMerge completed successfully!")
cat("\nMerged dataset saved to:", paste0(p$`1.data/b.merged`, "/morph_with_age.csv"))
cat("\nMerge report saved to:", paste0(p$`3.outputs`, "/age_merge_report.rds"))
```

## Visualization of Age Data Coverage
```{r Age Data Visualization}
# Create visualizations to understand the age data coverage

# 1. Distribution of specimens across stratigraphic levels
if(sum(!is.na(morph_with_age$CSTRAT)) > 0) {
  
  p1 <- ggplot(morph_with_age %>% filter(!is.na(CSTRAT)), 
               aes(x = CSTRAT)) +
    geom_histogram(bins = 30, fill = "skyblue", color = "black") +
    labs(title = "Distribution of Specimens by Stratigraphic Level (CSTRAT)",
         x = "Stratigraphic Level (cm)",
         y = "Number of Specimens") +
    theme_minimal()
  
  print(p1)
  
  # 2. Age distribution
  p2 <- ggplot(morph_with_age %>% filter(!is.na(YEAR)), 
               aes(x = YEAR)) +
    geom_histogram(bins = 30, fill = "lightgreen", color = "black") +
    labs(title = "Distribution of Specimens by Age (YEAR)",
         x = "Age (years)",
         y = "Number of Specimens") +
    theme_minimal() +
    scale_x_continuous(labels = scales::comma)
  
  print(p2)
  
  # 3. Relationship between CSTRAT and ISTRAT
  if(sum(!is.na(morph_with_age$ISTRAT)) > 0) {
    p3 <- ggplot(morph_with_age %>% filter(!is.na(CSTRAT) & !is.na(ISTRAT)), 
                 aes(x = CSTRAT, y = ISTRAT)) +
      geom_point(alpha = 0.6) +
      geom_smooth(method = "lm", se = TRUE, color = "red") +
      labs(title = "Relationship between CSTRAT and ISTRAT",
           x = "CSTRAT (cm)",
           y = "ISTRAT (cm)") +
      theme_minimal()
    
    print(p3)
  }
}
```

## Trait Evolution Over Time
```{r Trait Evolution Visualization}
# Get the continuous variables for trait evolution analysis
var_map <- variable_mapping()
continuous_vars <- var_map$continuous

# Prepare data for trait evolution analysis
trait_evolution_data <- morph_with_age %>%
  # Only include specimens with age data
  filter(!is.na(YEAR) & !is.na(CSTRAT)) %>%
  # Convert to long format for easier plotting
  pivot_longer(
    cols = all_of(continuous_vars),
    names_to = "trait",
    values_to = "value"
  ) %>%
  # Remove rows with missing trait values
  filter(!is.na(value)) %>%
  # Calculate sample sizes per trait for reference
  group_by(trait) %>%
  mutate(
    n_specimens = n(),
    trait_label = paste0(trait, " (n=", n_specimens, ")")
  ) %>%
  ungroup()

# Basic age distribution of specimens with trait data
p_age_dist <- ggplot(morph_with_age %>% filter(!is.na(YEAR)), 
                     aes(x = YEAR)) +
  geom_histogram(bins = 30, fill = "lightblue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Specimens with Age Data",
       x = "Age (years)",
       y = "Number of Specimens") +
  theme_minimal() +
  scale_x_continuous(labels = scales::comma)

print(p_age_dist)

# Multi-panel trait evolution over time (using YEAR)
if(nrow(trait_evolution_data) > 0) {
  
  p_traits_time <- ggplot(trait_evolution_data, aes(x = YEAR, y = value)) +
    geom_point(alpha = 0.4, size = 0.8) +
    geom_smooth(method = "loess", se = TRUE, color = "red", size = 0.8) +
    facet_wrap(~ trait_label, scales = "free_y", ncol = 3) +
    labs(title = "Morphological Trait Evolution Over Time",
         x = "Age (years)",
         y = "Trait Value") +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1),
      strip.text = element_text(size = 10)
    ) +
    scale_x_continuous(labels = scales::comma)
  
  print(p_traits_time)
  
  # Alternative view: traits over stratigraphic position (CSTRAT)
  p_traits_strat <- ggplot(trait_evolution_data, aes(x = CSTRAT, y = value)) +
    geom_point(alpha = 0.4, size = 0.8) +
    geom_smooth(method = "loess", se = TRUE, color = "blue", size = 0.8) +
    facet_wrap(~ trait_label, scales = "free_y", ncol = 3) +
    labs(title = "Morphological Traits Over Stratigraphic Position",
         x = "Stratigraphic Level (CSTRAT, cm)",
         y = "Trait Value") +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1),
      strip.text = element_text(size = 10)
    )
  
  print(p_traits_strat)
  
  # Focus on body size (SL) evolution if available
  sl_data <- trait_evolution_data %>% filter(trait == "SL")
  
  if(nrow(sl_data) > 10) {  # Only if we have reasonable sample size
    p_sl_evolution <- ggplot(sl_data, aes(x = YEAR, y = value)) +
      geom_point(alpha = 0.6, size = 1.2, color = "darkblue") +
      geom_smooth(method = "loess", se = TRUE, color = "red", fill = "pink") +
      labs(title = "Body Size (Standard Length) Evolution Over Time",
           x = "Age (years)",
           y = "Standard Length (SL)",
           caption = paste("n =", nrow(sl_data), "specimens")) +
      theme_minimal() +
      scale_x_continuous(labels = scales::comma)
    
    print(p_sl_evolution)
  }
  
  # Summary statistics for trait trends
  trait_trends <- trait_evolution_data %>%
    group_by(trait) %>%
    summarise(
      n_specimens = n(),
      age_range_years = max(YEAR) - min(YEAR),
      correlation_with_age = cor(YEAR, value, use = "complete.obs"),
      mean_value = mean(value),
      sd_value = sd(value),
      .groups = "drop"
    ) %>%
    arrange(desc(abs(correlation_with_age)))
  
  cat("\nTrait correlations with age (strongest correlations first):\n")
  print(trait_trends)
  
} else {
  cat("No trait evolution data available - check if specimens have both age and trait measurements.\n")
}

```
#==============================================================================#


#==============================================================================#
# PALEO ECO DATABASE
Transform tidy paleo data to rioja-compatible format
```{r}
paleo <- read.csv(paste0(p$`1.data/a.raw`, "060625_paleoeco_seriesL.csv"))
str(paleo)
```


```{r}
# FUNCTION 1: Explore your data structure
explore_paleo <- function(paleo) {
  cat("=== YOUR PALEO DATA OVERVIEW ===\n")
  cat("Total rows:", nrow(paleo), "\n")
  cat("Unique samples:", length(unique(paleo$Sample_ID)), "\n")
  cat("Microfossil types:", paste(unique(paleo$Microfossil_Type), collapse = ", "), "\n")
  cat("Date range of samples:", min(paleo$Sample_ID), "to", max(paleo$Sample_ID), "\n")
  
  # Check count distribution
  cat("\nCount summary:\n")
  print(summary(paleo$Count))
  
  # Sample counts by type
  cat("\nCounts by microfossil type:\n")
  type_counts <- paleo %>%
    group_by(Microfossil_Type) %>%
    summarise(
      total_specimens = sum(Count),
      n_samples = n_distinct(Sample_ID),
      .groups = "drop"
    )
  print(type_counts)
  
  # Extract and show stratigraphic levels
  strat_levels <- paleo %>%
    distinct(Sample_ID) %>%
    mutate(
      strat_number = as.numeric(str_extract(Sample_ID, "(?<=L)\\d+"))
    ) %>%
    filter(!is.na(strat_number)) %>%
    arrange(strat_number)
  
  cat("\nStratigraphic range: L", min(strat_levels$strat_number), 
      " to L", max(strat_levels$strat_number), "\n")
  cat("Number of stratigraphic levels:", nrow(strat_levels), "\n")
  
  return(strat_levels)
}

# FUNCTION 2: Convert your data to rioja format
convert_to_rioja <- function(paleo, microfossil_type = "Diatom", 
                           min_total_count = 10, min_samples = 2) {
  
  cat("=== CONVERTING TO RIOJA FORMAT ===\n")
  cat("Focusing on:", microfossil_type, "\n")
  cat("Minimum total count per taxon:", min_total_count, "\n")
  cat("Minimum samples per taxon:", min_samples, "\n")
  
  # Step 1: Filter to your chosen microfossil type and clean data
  paleo_clean <- paleo %>%
    filter(Microfossil_Type == microfossil_type) %>%
    # Create clean taxon names
    mutate(
      # Build taxon identifier from available taxonomy
      Taxon = case_when(
        !is.na(Genus_Type) & !is.na(Species) & 
        Genus_Type != "" & Species != "" ~ paste(Genus_Type, Species, sep = "_"),
        !is.na(Genus_Type) & Genus_Type != "" ~ paste(Genus_Type, "spp", sep = "_"),
        !is.na(Morphotype) & Morphotype != "" ~ Morphotype,
        TRUE ~ "Unknown"
      ),
      # Extract stratigraphic level number
      strat_level = as.numeric(str_extract(Sample_ID, "(?<=L)\\d+"))
    ) %>%
    # Remove zero counts and problematic entries
    filter(Count > 0, !is.na(strat_level), Taxon != "Unknown")
  
  # Step 2: Filter out rare taxa
  common_taxa <- paleo_clean %>%
    group_by(Taxon) %>%
    summarise(
      total_count = sum(Count),
      n_samples = n_distinct(Sample_ID),
      .groups = "drop"
    ) %>%
    filter(total_count >= min_total_count, n_samples >= min_samples)
  
  cat("Started with", length(unique(paleo_clean$Taxon)), "taxa\n")
  cat("Kept", nrow(common_taxa), "common taxa after filtering\n")
  
  # Step 3: Convert to wide format
  paleo_wide <- paleo_clean %>%
    filter(Taxon %in% common_taxa$Taxon) %>%
    # Sum counts if there are duplicates within a sample
    group_by(Sample_ID, Taxon, strat_level) %>%
    summarise(Count = sum(Count), .groups = "drop") %>%
    # Pivot to wide format
    pivot_wider(
      names_from = Taxon,
      values_from = Count,
      values_fill = 0
    ) %>%
    # Sort by stratigraphic level (oldest first)
    arrange(desc(strat_level))
  
  # Step 4: Separate sample info from count data
  sample_info <- paleo_wide %>%
    select(Sample_ID, strat_level)
  
  count_matrix <- paleo_wide %>%
    select(-Sample_ID, -strat_level) %>%
    as.data.frame()
  
  # Set row names for rioja
  rownames(count_matrix) <- sample_info$Sample_ID
  
  cat("Final dataset:", nrow(count_matrix), "samples ×", ncol(count_matrix), "taxa\n")
  
  # Return everything you need
  return(list(
    counts = count_matrix,
    samples = sample_info,
    taxa_info = common_taxa,
    microfossil_type = microfossil_type
  ))
}

# FUNCTION 3: Create stratigraphic plot (rioja version)
create_strat_plot <- function(rioja_data, use_percentages = TRUE, 
                            plot_title = NULL) {
  
  cat("=== CREATING STRATIGRAPHIC PLOT (RIOJA) ===\n")
  
  # Prepare data
  plot_data <- rioja_data$counts
  depths <- rioja_data$samples$strat_level
  
  if (use_percentages) {
    plot_data <- plot_data / rowSums(plot_data) * 100
    cat("Using percentages\n")
  } else {
    cat("Using raw counts\n")
  }
  
  # Set up plot title
  if (is.null(plot_title)) {
    plot_title <- paste(rioja_data$microfossil_type, "Stratigraphy")
  }
  
  # Create the plot with absolute minimal parameters
  par(mar = c(5, 4, 4, 8))
  
  # Try the most basic strat.plot call possible
  try({
    strat.plot(plot_data, yvar = depths)
    title(main = plot_title)
  }, silent = FALSE)
  
  cat("Plot created successfully!\n")
  return(plot_data)
}

# FUNCTION 3B: Create proper stratigraphic plot (ggplot version)
create_proper_strat_plot <- function(rioja_data, use_percentages = TRUE, 
                                   plot_title = NULL) {
  
  cat("=== CREATING PROPER STRATIGRAPHIC PLOT (GGPLOT) ===\n")
  
  plot_data <- rioja_data$counts
  samples <- rioja_data$samples
  
  if (use_percentages) {
    plot_data <- plot_data / rowSums(plot_data) * 100
    cat("Using percentages\n")
  } else {
    cat("Using raw counts\n")
  }
  
  # Set up plot title
  if (is.null(plot_title)) {
    plot_title <- paste(rioja_data$microfossil_type, "Stratigraphy")
  }
  
  # Convert to long format
  plot_long <- plot_data %>%
    bind_cols(samples) %>%
    pivot_longer(cols = -c(Sample_ID, strat_level), 
                 names_to = "Taxon", values_to = "Abundance")
  
  # Create taxon positions for side-by-side layout
  taxa_order <- names(plot_data)
  plot_long$taxon_position <- match(plot_long$Taxon, taxa_order)
  
  # Calculate scaled abundance for plotting (each taxon gets equal width)
  plot_long <- plot_long %>%
    group_by(Taxon) %>%
    mutate(
      max_abundance = max(Abundance, na.rm = TRUE),
      scaled_abundance = if_else(max_abundance > 0, Abundance / max_abundance * 0.8, 0),
      x_start = taxon_position - 0.4,
      x_end = taxon_position - 0.4 + scaled_abundance
    ) %>%
    ungroup()
  
  # Create the stratigraphic plot
  p <- ggplot(plot_long) +
    # Add the abundance curves
    geom_ribbon(aes(xmin = x_start, xmax = x_end, y = strat_level, group = Taxon),
                fill = "lightblue", color = "black", alpha = 0.7, size = 0.3) +
    # Add vertical lines at each taxon position
    geom_vline(xintercept = 1:length(taxa_order) - 0.4, 
               color = "gray50", linetype = "dashed", alpha = 0.5) +
    # Customize axes
    scale_x_continuous(
      breaks = 1:length(taxa_order),
      labels = taxa_order,
      expand = c(0.02, 0)
    ) +
    scale_y_continuous(expand = c(0.01, 0)) +
    # Labels and theme
    labs(
      title = plot_title,
      x = "Taxa",
      y = "Stratigraphic Level",
      caption = if(use_percentages) "Abundance shown as % of total" else "Abundance (counts)"
    ) +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
      axis.text.y = element_text(size = 9),
      plot.title = element_text(size = 12, hjust = 0.5),
      panel.grid.minor = element_blank(),
      panel.grid.major.x = element_blank()
    )
  
  print(p)
  cat("Proper stratigraphic plot created!\n")
  return(p)
}

# FUNCTION 4: Calculate diversity metrics
calculate_diversity <- function(rioja_data) {
  
  cat("=== CALCULATING DIVERSITY METRICS ===\n")
  
  counts <- rioja_data$counts
  samples <- rioja_data$samples
  
  diversity_data <- data.frame(
    Sample_ID = samples$Sample_ID,
    strat_level = samples$strat_level,
    richness = apply(counts > 0, 1, sum),        # Number of taxa
    total_count = rowSums(counts),               # Total specimens
    shannon = diversity(counts, index = "shannon"),  # Shannon diversity
    simpson = diversity(counts, index = "simpson")   # Simpson diversity
  ) %>%
    arrange(desc(strat_level))  # Oldest first
  
  cat("Diversity calculated for", nrow(diversity_data), "samples\n")
  return(diversity_data)
}

# FUNCTION 5: Plot diversity trends
plot_diversity <- function(diversity_data, metric = "shannon") {
  
  cat("=== PLOTTING DIVERSITY TRENDS ===\n")
  
  if (metric == "shannon") {
    p <- ggplot(diversity_data, aes(x = shannon, y = strat_level)) +
      labs(x = "Shannon Diversity", title = "Shannon Diversity Through Section")
  } else if (metric == "richness") {
    p <- ggplot(diversity_data, aes(x = richness, y = strat_level)) +
      labs(x = "Species Richness", title = "Species Richness Through Section")
  } else if (metric == "total_count") {
    p <- ggplot(diversity_data, aes(x = total_count, y = strat_level)) +
      labs(x = "Total Count", title = "Total Specimen Count Through Section")
  }
  
  p <- p +
    geom_line(color = "blue") +
    geom_point(size = 2, color = "darkblue") +
    labs(y = "Stratigraphic Level") +
    theme_minimal() +
    theme(axis.text = element_text(size = 10))
  
  print(p)
  return(p)
}

```

```{r}
create_simple_plot <- function(rioja_data, use_percentages = TRUE) {
  
  plot_data <- rioja_data$counts
  samples <- rioja_data$samples
  
  if (use_percentages) {
    plot_data <- plot_data / rowSums(plot_data) * 100
  }
  
  # Convert to long format for ggplot
  plot_long <- plot_data %>%
    bind_cols(samples) %>%
    pivot_longer(cols = -c(Sample_ID, strat_level), 
                 names_to = "Taxon", values_to = "Abundance") %>%
    filter(Abundance > 0)  # Only show non-zero values
  
  # Create the plot
  p <- ggplot(plot_long, aes(x = Abundance, y = strat_level)) +
    geom_area(fill = "lightblue", alpha = 0.7) +
    facet_wrap(~ Taxon, scales = "free_x") +
    labs(y = "Stratigraphic Level", 
         x = if(use_percentages) "Percentage" else "Count",
         title = paste(rioja_data$microfossil_type, "Stratigraphy")) +
    theme_minimal() +
    theme(strip.text = element_text(size = 8))
  
  print(p)
  return(p)
}

# FUNCTION 3: Create stratigraphic plot
create_strat_plot <- function(rioja_data, use_percentages = TRUE, 
                            plot_title = NULL) {
  
  cat("=== CREATING STRATIGRAPHIC PLOT ===\n")
  
  # Prepare data
  plot_data <- rioja_data$counts
  depths <- rioja_data$samples$strat_level
  
  if (use_percentages) {
    plot_data <- plot_data / rowSums(plot_data) * 100
    cat("Using percentages\n")
  } else {
    cat("Using raw counts\n")
  }
  
  # Set up plot title
  if (is.null(plot_title)) {
    plot_title <- paste(rioja_data$microfossil_type, "Stratigraphy")
  }
  
  # Create the plot with simplified parameters
  par(mar = c(5, 4, 4, 8))  # Adjust margins
  
  # Use basic strat.plot with minimal parameters to avoid conflicts
  strat.plot(
    plot_data,
    yvar = depths,
    scale.percent = use_percentages,
    plot.poly = TRUE,
    col.poly = "lightblue",
    main = plot_title,
    ylab = "Stratigraphic Level"
  )
  
  cat("Plot created successfully!\n")
  return(plot_data)
}

```

```{r}
# STEP 2: Explore the data
strat_info <- explore_paleo(paleo)

# STEP 3: Convert to rioja format (start with diatoms)
rioja_data <- convert_to_rioja(paleo,
                              microfossil_type = "Diatom",
                              min_total_count = 10,
                              min_samples = 2)

# STEP 4: Create stratigraphic plot
strat_plot_data <- create_strat_plot(rioja_data)

# OR if that fails, use the ggplot version
proper_strat_plot <- create_proper_strat_plot(rioja_data, use_percentages = TRUE)

# STEP 5: Calculate and plot diversity
diversity_metrics <- calculate_diversity(rioja_data)
plot_diversity(diversity_metrics, metric = "shannon")
plot_diversity(diversity_metrics, metric = "richness")

# STEP 6: Save your results
write.csv(rioja_data$counts, "diatom_counts_wide.csv", row.names = TRUE)
write.csv(strat_plot_data, "diatom_percentages.csv", row.names = TRUE)
write.csv(diversity_metrics, "diversity_metrics.csv", row.names = FALSE)

```








#==============================================================================#
#SCRAP
## Creating morph_with_pgna
```{r PGNA, eval=FALSE, include=FALSE}
morph <- read.csv(paste0(p$`1.data/a.raw`, "241015_PitLMorph.csv"))
pgna <- read.csv(paste0(p$`1.data/a.raw`, "PGNA_PitLMorph.csv"))

# First, let's understand the tpgna duplicates
pgna_summary <- tpgna %>%
  group_by(ID) %>%
  summarise(
    count = n(),
    unique_values = n_distinct(PGNA),
    min_pgna = min(PGNA),
    max_pgna = max(PGNA),
    .groups = "drop"
  )

# Check if there are conflicting values within the same ID
conflicting_pgna <- pgna_summary %>%
  filter(unique_values > 1)

# Aggregate tpgna (taking max value for binary flags, similar to your other binary vars)
pgna_agg <- pgna %>%
  group_by(ID) %>%
  summarise(
    PGNA = max(PGNA, na.rm = TRUE),
    .groups = "drop"
  )

# Now join with the aggregated data
morph_with_pgna <- morph %>%
  left_join(pgna_agg, by = "ID")

write.csv(morph_with_pgna, file = paste0(p$`1.data/a.raw`, "020625_PitLMorph.csv"), row.names = FALSE)

```
