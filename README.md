# morphStuart: Stickleback Morphology and Paleo-Ecology Analysis Pipeline

This project implements a data processing and analysis pipeline for stickleback morphological measurements and associated paleo-ecological microfossil data. The goal is to integrate these diverse datasets to facilitate evolutionary and ecological research.

The pipeline is built using the `targets` R package, which ensures reproducibility, efficient re-execution of steps, and clear tracking of data dependencies.

## Project Structure

*   `_targets.R`: The main script defining the data processing and analysis pipeline using `targets`.
*   `R/`: Contains R scripts with modular functions for data import, cleaning, processing, analysis, and visualization.
*   `data/`: Stores raw input data and will contain processed and flagged data outputs.
    *   `data/raw/`: Original input CSV files.
    *   `data/processed/`: Cleaned and processed data generated by the pipeline.
    *   `data/flagged/`: Data records identified as problematic during QC steps.
*   `results/`: Stores final analysis outputs, plots, and reports.
    *   `results/plots/`: Generated visualizations.
    *   `results/report/`: Rendered analysis reports.
*   `config.yml`: Configuration file for setting input/output paths, analysis parameters, and plot dimensions.
*   `renv.lock`: Records the exact R package versions used in the project for reproducibility.

## Dependencies

*   **R (>= 4.0)**: The statistical programming language.
*   **R packages**: Managed by `renv`. Key packages include `targets`, `dplyr`, `tidyr`, `ggplot2`, `vegan`, `rioja`, and `knitr`.

## Setup and Initialization

To set up the R environment and install all necessary package dependencies at their specified versions, run the following command in your R console from the project root directory:

```R
renv::restore()
```

This will read the `renv.lock` file and install the required packages into a project-specific library.

## Configuration

The `config.yml` file allows you to customize various aspects of the pipeline, including:

*   **`paths`**: File paths for raw data inputs, and directories for processed data, plots, and reports.
*   **`analysis`**: Parameters for data cleaning (e.g., `conflict_threshold`), taxonomic aggregation (`rioja_grouping_level`), rare taxa filtering (`rare_taxa_threshold`), community metrics (`community_evenness_index`, `time_column`), and turnover analysis (`turnover_method`).
*   **`plots`**: Dimensions for generated plots.

Review and adjust `config.yml` as needed before running the pipeline.

## Running the Pipeline

The entire data processing and analysis workflow is managed by the `targets` package. To execute the pipeline, run the following command in your R console from the project root directory:

```R
targets::tar_make()
```

`tar_make()` will:
1.  Identify all steps (targets) defined in `_targets.R`.
2.  Determine which targets need to be run or re-run based on changes in code or data.
3.  Execute only the necessary steps, saving intermediate and final results.

## Viewing the Pipeline

You can visualize the entire `targets` workflow, including dependencies between steps, by running:

```R
targets::tar_visnetwork()
```

This will open an interactive network graph in your browser, providing a clear overview of the pipeline's structure.

To check the progress of the pipeline, use:

```R
targets::tar_progress()
```

## Outputs

Upon successful completion, the pipeline will generate:

*   **Processed Data**: Cleaned and transformed datasets in `data/processed/`.
*   **Flagged Data**: Records identified as problematic during quality control in `data/flagged/`.
*   **Plots**: Various visualizations of data quality, distributions, and analytical results in `results/plots/`.
*   **Reports**: Comprehensive analysis reports (e.g., HTML, PDF) in `results/report/`.